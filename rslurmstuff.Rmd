---
title: "rslurm and getafix"
author: "Patrick Sykes"
date: "16/07/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## The `rslurm` package

### Submitting a job with rslurm

Package `rslurm` allows you to produce the files to run a parallel job on a SLURM queue and extract the results. The basic syntax to produce the submission files is below.  

```
slr_job <- slurm_apply(f, params, jobname = NA, nodes = 1, cpus_per_node = 1,add_objects = NULL, pkgs = rev(.packages()), libPaths = NULL,slurm_options = list(), submit = TRUE)
```

* `f` is a function containing code you want to run. It should take one or many single values as parameters, and may return any type of R object.

* `params` is a data frame of parameter values to apply `f` to. Each column corresponds to a parameter of `f` (note that the names must agree) and each row corresponds to a separate function call.

* `jobname` is the name of the Slurm job. If this is set to `NA`, the job name is "slr#####, where #### is 4 random digits.

* `nodes` is the (maximum) number of cluster nodes to be used. `slurm_apply` automatically divides the rows of `params` into chunks of approximately equal size. Fewer nodes are allocated if the parameter set is too small to use all CPUs on the requested nodes.

* `cpus_per_node` determines how many processes will be run in parallel on each node.

* `add_objects` is a character vector containing the names of R objects to be saved in a .RData file and loaded on each node prior to calling `f`.

* `pkgs` is a character vector containing the names of packages that must be loaded on each node. Default behaviour is to use all packages loaded when you call `slurm_apply`.

* `libPaths` is a character vector with the locations of additional R library trees to search through. If `NULL`, it only looks in the libraries returned by `.libPaths()` on the node. Non-existent libraries are silently ignored.

* `slurm options` is a list of options recognised by SLURM's `sbatch`. Note that the "array", "job-name", "nodes" and "output" are determined by `slurm_apply`, so don't specify them here.

  + `time` is the time limit on each job, in the format `D-HH:MM`.

  + `mem-per-cpu` is the memory to be allocated to each cpu being used.

  + `ntasks` refers to the number of tasks in each job. This is **not** the total number of tasks. Usually this will be the default value of 1.

  + `cpus-per-task` sets the number of cpus to be allocated per process. Default is 1.

  + Other options can be found in the `sbatch` [documentation](https://slurm.schedmd.com/sbatch.html). Note that the full option names must be used (for example, `time` rather than `t`).

* `submit` determines whether or not the job is sent ot the cluster. If `submit = TRUE`, the job is sent to the cluster and either a confirmation or error message is output to the console. When `submit = FALSE`, a message indicates the location of the saved data and script files and you can submit the job manually by running `sbatch submit.sh` from that directory.

### Read the output

The results will be saved as a collection of files `results_#.RDS`, one file for each cluster node used. The `get_slurm_out` reads all these files and returns the result as a single data frame or list.

```
func_result <- get_slurm_out(slr_job, outtype = "raw", wait = TRUE)
```

Here, `slr_job` is a SLURM job (the output of a `slurm_apply` or `slurm_job` command), `outtype` specifies whether the output should be a list (`outtype="raw"`) or data frame (`outtype="table"`) and `wait` specifies whether or not the command should be blocked until `slr_job` is complete.

`slr_job` is automatically created when `slurm_apply` is called, but if you change R session you may need to manually recreate it to use `get_slurm_out`. Use the following code to do so, using the same `jobname` and number of `nodes` as the submitted job.

```
slr_job <- slurm_job(jobname, nodes)
```

### Other rslurm commands

If running R on the cluster, you can use the following commands to check on the status of a running job (or the console and error output generated, if any, of a completed one), or cancel a running job. 

```
print_job_status(slr_job)
cancel_slurm(slr_job)
```

Once the results have been collected with `get_slurm_out`, you can use `cleanup_files(slr_job)` to delete the temporary files and raw outputs from the cluster.

To run a single function evaluation on the cluster, you can use `slurm_call` in place of `slurm_apply`:

```
slr_job <- slurm_call(f, params, jobname = NA, add_objects = NULL,pkgs = rev(.packages()), libPaths = NULL, slurm_options = list(),submit = TRUE)
```

### Issues

* The submit.sh file that rslurm ouputs by default doesn't work on the cluster. To fix this, the authors of the package recommend editing the template file. Go to your R library (use `.libPaths()` in the console to find out where this is) and then to the rslurm/templates/submit_sh.txt. Replace line 12 with

```
module load R
R CMD BATCH slurm_run.R
```

* For large arrays (more than ~150 tasks), you should be limiting the number of jobs being requested at one time, but this isn't possible if you submit directly from R with `submit=TRUE` in `slurm_apply`. Make sure you submit your job array manually to set the right array (i.e. `sbatch --array=0-<max_node>%100 submit.sh` to limit to 100 tasks.)

* If you generate the submission files in Windows, it may not play nicely with the cluster (which expects a *NIX-formatted bash script). Run `dos2unix submit.sh` first to convert submit.sh to the correct formatting.

## Accounting with `sacct`

It can be hard to know in advance what the memory and time requirements of a given job are, and it can take some trial-and-error. My recommendation is to run the first time with "plenty" of resources (e.g. multiply your best esimate by two) then use the actual usage to determine what you need in future trials.

You can type `sacct -j <jobid> --format=JobID,JobName,MaxRSS,Elapsed` into the terminal to find out the maximum memory usage and elapsed time of a particular job. If you don't know the jobid, use `--starttime YYYY-MM-DD` and `--endtime YYYY-MM-DD` to show all jobs run between two dates. For more options, see the `sacct` [documentation](https://slurm.schedmd.com/sacct.html).